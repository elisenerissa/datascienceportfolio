{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-NUL0X8Zc9F"
   },
   "source": [
    "For introduction and problem statement, please refer to notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbqyC_u8Zc9G"
   },
   "source": [
    "## Content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uG5atca0Zc9H"
   },
   "source": [
    "Notebook 1: 1_cellphones_reviews_data_cleaning_and_eda\n",
    "- Data Import and Cleaning\n",
    "- Exploratory Data Analysis\n",
    "- Text Data Pre-processing\n",
    "\n",
    "**Notebook 2: 2_cellphones_reviews_topic modelling**\n",
    "- Data Import\n",
    "- Topic Modelling with Gensim\n",
    "\n",
    "**Notebook 3: 3_cellphones_reviews_topic_analysis_and_visualizations**\n",
    "- Findings and Analysis of Topic Modelling\n",
    "\n",
    "**Notebook 4: 4_features_extractions_and_sentiment_analysis**\n",
    "- Data Import\n",
    "- Sentiment Analysis with VADER\n",
    "- Sentiment Analysis with Logistic Regression(Multi-Class Classification)\n",
    "- Evaluation of Sentiment Analysis with BERT(Multi-Class Classification)\n",
    "Please refer to notebook 5 for the fine-tuning process of pre-trained BERT model\n",
    "\n",
    "\n",
    "**Notebook 5: 5_fine_tuning_of_BERT_model**   \n",
    "The reason why this notebook is separated from notebook 4 which contains the evaluation of BERT model is because the fine-tuning of BERT model requires GPU. Hence, the model was fine-tuned on Google Colaboratory and loaded back into notebook 4 for evaluation\n",
    "\n",
    "\n",
    "**Notebook 6: 6_analysis_and_findings**\n",
    "- [Data Import](#Data-Import)\n",
    "- [Comparison of the 3 Methods](#Evaluating-and-Comparing-the-3-Models)\n",
    "- [Deployment](#Deployment)\n",
    "- [Conclusion and Future Steps](#Conclusion-and-Future-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWrSxTbwZc9H"
   },
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5ktsLEyZc9I"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk import tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9K14PL5Zc9L"
   },
   "outputs": [],
   "source": [
    "new_reviews  = pickle.load(open('../data/reviews_with_feature_sentiments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section, we are just copying the functions that were defined in the earlier notebook so that we can make predictions on toy examples to compare the 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#instantiate vader sentiment analyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "#adding a new word to the lexicon\n",
    "new_words = {\n",
    "    'new': 3.0\n",
    "}\n",
    "\n",
    "analyser.lexicon.update(new_words)\n",
    "\n",
    "#defining the stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "#remove negation words from stop words as they are useful context for sentiment predictions\n",
    "negation_words = ['ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "\"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
    "'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "'wouldn', \"wouldn't\",\"not\",\"no\",'don',\"don't\"]\n",
    "\n",
    "for word in negation_words:\n",
    "    stop_words.remove(word)\n",
    "\n",
    "#confirm that the negation words have been removed\n",
    "len(stop_words)\n",
    "\n",
    "def summarise_reviews (reviews):\n",
    "    \"\"\"\n",
    "    Find sentences with keywords, followed by cleaning by removing html, non letter words, stopwords and lemmatizing\n",
    "\n",
    "    Parameter\n",
    "    ----------\n",
    "    reviews:a string of words \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary: processed sentences that contained keywords defined below\n",
    "    \"\"\"\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen',\n",
    "                        'fingerprint','fingerprints','ringtones','charger']\n",
    "    summary = set()\n",
    "    texts = tokenize.sent_tokenize(reviews)\n",
    "    for sentence in texts:\n",
    "        sentence = sentence.lower()\n",
    "        for word in list_of_keywords:\n",
    "            if word in sentence:\n",
    "                # Remove HTML.\n",
    "                post_text = BeautifulSoup(sentence).get_text()\n",
    "\n",
    "                # Remove non-letters.\n",
    "                letters_only = ' '.join(re.findall(r\"[A-z’]+\",post_text))\n",
    "\n",
    "                # Convert to lower case, split into individual words.\n",
    "                words = letters_only.lower().split()\n",
    "\n",
    "                #convert the stopwords to a set.\n",
    "                stops = set(stop_words)\n",
    "\n",
    "                # Remove stopwords.\n",
    "                meaningful_words = [w for w in words if w not in stops]\n",
    "\n",
    "                # Stemming \n",
    "                #p_stemmer = PorterStemmer()\n",
    "                #meaningful_words = [p_stemmer.stem(w) for w in meaningful_words]\n",
    "\n",
    "                #Lemmatize\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                meaningful_words = [lemmatizer.lemmatize(word) for word in meaningful_words]\n",
    "\n",
    "                cleaned_sentence = (\" \".join(meaningful_words))\n",
    "                \n",
    "                summary.add(cleaned_sentence)\n",
    "                \n",
    "    return list(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiments (reviews):\n",
    "    \"\"\"\n",
    "    extract features by searching for sentences with keywords defined \n",
    "    and predicting sentiments of each sentence using vader \n",
    "\n",
    "    Parameter\n",
    "    ----------\n",
    "    summarised_reviews: a string of words (reviews that have been cleaned)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (sentiment score, keyword)\n",
    "    \"\"\"\n",
    "    #list down the keywords\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen','fingerprint',\n",
    "                        'fingerprints','ringtones','charger']\n",
    "    summary = set()\n",
    "    \n",
    "    summarised_reviews = summarise_reviews (reviews)\n",
    "    #loop through each sentence to make predictions\n",
    "    for cleaned_sentence in summarised_reviews:\n",
    "        \n",
    "        #only predict and keep sentences with keywords\n",
    "        for word in list_of_keywords:\n",
    "            if word in cleaned_sentence:\n",
    "                #predict sentiment with vader\n",
    "                score = analyser.polarity_scores(cleaned_sentence)\n",
    "                compound = score['compound']\n",
    "                #assign negative sentiment to 1, \n",
    "                #neutral to 3, positive to 5\n",
    "                if compound >= 0.05:\n",
    "                    sentiment_score = 5\n",
    "                elif compound >= -0.05:\n",
    "                    sentiment_score = 3\n",
    "                else:\n",
    "                    sentiment_score = 1\n",
    "\n",
    "                summary.add((sentiment_score,word))\n",
    "    return list(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = pickle.load(open('../data/logreg_3classes.pkl', 'rb'))\n",
    "\n",
    "def logreg_sentiments(reviews):\n",
    "    \"\"\"\n",
    "    extract features by searching for sentences with keywords defined \n",
    "    and predicting sentiments of each sentence using logistic regression\n",
    "\n",
    "    Parameter\n",
    "    ----------\n",
    "    summarised_reviews: a string of words (reviews that have been cleaned)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (sentiment score, keyword)\n",
    "    \n",
    "    \"\"\"\n",
    "    reviews = summarise_reviews (reviews)\n",
    "    \n",
    "    list_of_keywords = ['camera','screen','battery','simcard',\n",
    "                        'touchscreen','fingerprint','fingerprints','ringtones','charger']\n",
    "    summary = set()\n",
    "    #predict with logistic regression model\n",
    "    pred = logreg_model.predict(reviews)\n",
    "    \n",
    "    predicted_ratings= []\n",
    "    #convert class 0,1,2 to 1,3,5 \n",
    "    for score in pred:\n",
    "        if float(score) == 2.0:\n",
    "            rating = 5\n",
    "            predicted_ratings.append(rating)\n",
    "        elif float(score) == 1.0:\n",
    "            rating = 3\n",
    "            predicted_ratings.append(rating)\n",
    "        else:\n",
    "            rating = 1\n",
    "            predicted_ratings.append(rating)\n",
    "    \n",
    "    #loop through each clean sentence\n",
    "    for i,cleaned_sentence in enumerate(reviews):        \n",
    "        for word in list_of_keywords:\n",
    "            if word in cleaned_sentence:\n",
    "                summary.add((predicted_ratings[i],word))\n",
    "                \n",
    "    \n",
    "    return list(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../data/finetuned_BERT_epoch_2_3classes.model', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "### Loading Tokenizer and Encoding Data by Sentences\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "def sentences_with_keywords (reviews):\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen','fingerprint','fingerprints',\n",
    "                        'ringtones','charger']\n",
    "    summarised_reviews = set()\n",
    "    texts = tokenize.sent_tokenize(reviews)\n",
    "    for sentence in texts:\n",
    "        sentence = sentence.lower()\n",
    "        for word in list_of_keywords:\n",
    "            if word in sentence:\n",
    "                summarised_reviews.add(sentence)\n",
    "    \n",
    "    summarised_reviews = list(summarised_reviews)\n",
    "    \n",
    "    return summarised_reviews\n",
    "\n",
    "def bert_sentiments (reviews):\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen','fingerprint','fingerprints',\n",
    "                        'ringtones','charger']\n",
    "    \n",
    "    summary = set()\n",
    "    \n",
    "    summarised_reviews = sentences_with_keywords (reviews)\n",
    "    \n",
    "    encoded_data_features = tokenizer.batch_encode_plus(\n",
    "    summarised_reviews, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "    input_ids_features = encoded_data_features['input_ids']\n",
    "    attention_masks_features = encoded_data_features['attention_mask']\n",
    "    #labels_features = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "    dataset_features = TensorDataset(input_ids_features, attention_masks_features)\n",
    "\n",
    "    dataloader_features = DataLoader(dataset_features , \n",
    "                                       sampler=SequentialSampler(dataset_features ), \n",
    "                                       batch_size=batch_size)\n",
    "\n",
    "    #activate evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #loop through the data that is fed into the function\n",
    "    for batch in dataloader_features:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                     }\n",
    "    #deactivate gradiant calculation\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        \n",
    "    #calculate the predicted class by finding the index of the highest logit\n",
    "    rating_score = torch.argmax(outputs[0],dim=1)\n",
    "\n",
    "    #convert class 0,1,2 to 1,3,5 \n",
    "    try:\n",
    "        \n",
    "        predicted_ratings = []\n",
    "\n",
    "        for score in rating_score:\n",
    "            if float(score) == 2.0:\n",
    "                rating = 5\n",
    "                predicted_ratings.append(rating)\n",
    "            elif float(score) == 1.0:\n",
    "                rating = 3\n",
    "                predicted_ratings.append(rating)\n",
    "            else:\n",
    "                rating = 1\n",
    "                predicted_ratings.append(rating)\n",
    "        \n",
    "        #loop through each sentence with keyword\n",
    "        for i,cleaned_sentence in enumerate(summarised_reviews):        \n",
    "            for word in list_of_keywords:\n",
    "                if word in cleaned_sentence:\n",
    "                    summary.add((float(predicted_ratings[i]),word))\n",
    "    except:\n",
    "        summary.add(np.nan)\n",
    "        \n",
    "    return list(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and Comparing the 3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ratings (feature_ratings):\n",
    "    \"\"\"\n",
    "    calculate mean ratings for each feature if there are two features with different sentiments in a review\n",
    "\n",
    "    Parameter\n",
    "    ----------\n",
    "    [(sentiment score 1,feature 1),(sentiment score 2,feature 2),...] (multiples tuples grouped in a list format)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary with feature as key and mean sentiment as value\n",
    "    \n",
    "    \"\"\"\n",
    "    #define the dictionary\n",
    "    all_features = {'camera':[],'battery':[],'fingerprint':[],'screen':[],'charger':[],'simcard':[],'ringtones':[]}\n",
    "    \n",
    "    #iterate through the list of feature,sentiments tuple within a review\n",
    "    try:\n",
    "        for i,feature in enumerate(feature_ratings):\n",
    "\n",
    "                if feature[1]  == 'camera':\n",
    "                    all_features ['camera'].append(feature[0])\n",
    "\n",
    "                elif feature[1] =='battery':\n",
    "                    all_features ['battery'].append(feature[0])\n",
    "\n",
    "                elif feature[1] == 'fingerprint':\n",
    "                    all_features ['fingerprint'].append(feature[0])\n",
    "\n",
    "                elif feature[1] == 'fingerprints':\n",
    "                    all_features ['fingerprint'].append(feature[0])\n",
    "\n",
    "                elif feature[1] == 'screen':\n",
    "                    all_features ['screen'].append(feature[0])\n",
    "\n",
    "                elif feature[1]  == 'charger':\n",
    "                    all_features ['charger'].append(feature[0])\n",
    "\n",
    "                elif feature[1] == 'touchscreen':\n",
    "                    all_features ['screen'].append(feature[0])\n",
    "\n",
    "                elif feature[1] == 'simcard':\n",
    "                    all_features ['simcard'].append(feature[0])\n",
    "\n",
    "                elif feature[1] == 'ringtones':\n",
    "                    all_features ['ringtones'].append(feature[0])\n",
    "    \n",
    "    \n",
    "        #calculate the mean value \n",
    "        try: \n",
    "            all_features_mean = {key:np.mean(value) for key,value in all_features.items()}\n",
    "\n",
    "        except:\n",
    "            \n",
    "            all_features_mean = {key:np.nan for key,value in all_features.items()}\n",
    "        \n",
    "        #for keys with nan value, remove the key and value pair\n",
    "        new_dict = {key:val for key, val in all_features_mean.items() if math.isnan(val)==False}\n",
    "    \n",
    "    except:\n",
    "        new_dict = np.nan\n",
    "    \n",
    "    return new_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean ratings - if there are features that are mentioned twice with different sentiments, the mean \n",
    "#rating will be displayed\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    new_reviews['vader_analysis'] = new_reviews['vader_analysis'].apply(mean_ratings)\n",
    "    new_reviews['logreg_pred'] = new_reviews['logreg_pred'].apply(mean_ratings)\n",
    "    new_reviews['bert_analysis'] = new_reviews['bert_analysis'].map(mean_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEjsS9QqZc9Z",
    "outputId": "c684cc5b-7298-427d-ed09-896c1cb1efd9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vader_analysis</th>\n",
       "      <th>logreg_pred</th>\n",
       "      <th>bert_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>{'battery': 5.0, 'screen': 5.0}</td>\n",
       "      <td>{'battery': 5.0, 'screen': 5.0}</td>\n",
       "      <td>{'battery': 5.0, 'screen': 5.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10551</th>\n",
       "      <td>{'camera': 5.0, 'screen': 3.0}</td>\n",
       "      <td>{'camera': 5.0, 'screen': 3.0}</td>\n",
       "      <td>{'camera': 5.0, 'screen': 3.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12233</th>\n",
       "      <td>{'camera': 3.0}</td>\n",
       "      <td>{'camera': 1.0}</td>\n",
       "      <td>{'camera': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>{'screen': 5.0}</td>\n",
       "      <td>{'screen': 5.0}</td>\n",
       "      <td>{'screen': 5.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20801</th>\n",
       "      <td>{'fingerprint': 3.0}</td>\n",
       "      <td>{'fingerprint': 3.0}</td>\n",
       "      <td>{'fingerprint': 2.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18885</th>\n",
       "      <td>{'battery': 5.0}</td>\n",
       "      <td>{'battery': 5.0}</td>\n",
       "      <td>{'battery': 5.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20107</th>\n",
       "      <td>{'camera': 3.0, 'battery': 5.0, 'fingerprint': 4.0, 'screen': 4.0}</td>\n",
       "      <td>{'camera': 5.0, 'battery': 5.0, 'fingerprint': 5.0, 'screen': 5.0}</td>\n",
       "      <td>{'camera': 3.0, 'battery': 5.0, 'fingerprint': 5.0, 'screen': 5.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>{'camera': 5.0}</td>\n",
       "      <td>{'camera': 5.0}</td>\n",
       "      <td>{'camera': 5.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>{'battery': 3.0}</td>\n",
       "      <td>{'battery': 5.0}</td>\n",
       "      <td>{'battery': 5.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14257</th>\n",
       "      <td>{'battery': 3.0}</td>\n",
       "      <td>{'battery': 5.0}</td>\n",
       "      <td>{'battery': 5.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           vader_analysis  \\\n",
       "1617                                      {'battery': 5.0, 'screen': 5.0}   \n",
       "10551                                      {'camera': 5.0, 'screen': 3.0}   \n",
       "12233                                                     {'camera': 3.0}   \n",
       "477                                                       {'screen': 5.0}   \n",
       "20801                                                {'fingerprint': 3.0}   \n",
       "18885                                                    {'battery': 5.0}   \n",
       "20107  {'camera': 3.0, 'battery': 5.0, 'fingerprint': 4.0, 'screen': 4.0}   \n",
       "3576                                                      {'camera': 5.0}   \n",
       "5196                                                     {'battery': 3.0}   \n",
       "14257                                                    {'battery': 3.0}   \n",
       "\n",
       "                                                              logreg_pred  \\\n",
       "1617                                      {'battery': 5.0, 'screen': 5.0}   \n",
       "10551                                      {'camera': 5.0, 'screen': 3.0}   \n",
       "12233                                                     {'camera': 1.0}   \n",
       "477                                                       {'screen': 5.0}   \n",
       "20801                                                {'fingerprint': 3.0}   \n",
       "18885                                                    {'battery': 5.0}   \n",
       "20107  {'camera': 5.0, 'battery': 5.0, 'fingerprint': 5.0, 'screen': 5.0}   \n",
       "3576                                                      {'camera': 5.0}   \n",
       "5196                                                     {'battery': 5.0}   \n",
       "14257                                                    {'battery': 5.0}   \n",
       "\n",
       "                                                            bert_analysis  \n",
       "1617                                      {'battery': 5.0, 'screen': 5.0}  \n",
       "10551                                      {'camera': 5.0, 'screen': 3.0}  \n",
       "12233                                                     {'camera': 1.0}  \n",
       "477                                                       {'screen': 5.0}  \n",
       "20801                                                {'fingerprint': 2.0}  \n",
       "18885                                                    {'battery': 5.0}  \n",
       "20107  {'camera': 3.0, 'battery': 5.0, 'fingerprint': 5.0, 'screen': 5.0}  \n",
       "3576                                                      {'camera': 5.0}  \n",
       "5196                                                     {'battery': 5.0}  \n",
       "14257                                                    {'battery': 5.0}  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check to confirm that the mean rating was implemented correctly \n",
    "pd.set_option('display.max_colwidth',None)\n",
    "new_reviews[['vader_analysis','logreg_pred','bert_analysis']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the comments above, we are implementing this mean_rating function to all the output of the model predictions **because there are some reviews that mention a feature more than once with different sentiments.** Hence, in that case, the mean rating of that feature will be displayed instead.\n",
    "\n",
    "An example of how mean rating works is shown below. The predictions from VADER shows that **camera has a rating of 4. Our initial model output only had 3 classes: Class 0,1,2 which was subsequently converted to 1,3,5 (corresponds to negative, neutral and positive). However, the reason why the predicted rating is shown as 4 here is because VADER picked up two different sentiments (3 and 5) in the two sentences that talk about camera.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: ['the only con was the camera is not a $800.', 'i think the camera does a good job.']\n",
      "VADER predictions: {'camera': 4.0}\n"
     ]
    }
   ],
   "source": [
    "example = new_reviews.loc[19551,\"sentences_with_keywords\"]\n",
    "pred = new_reviews.loc[19551,\"vader_analysis\"]\n",
    "print(f\"Example: {example}\")\n",
    "print(f\"VADER predictions: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not have labels for sentiments at feature level, I have decided to do a manual accuracy test. I have extracted 40 reviews, sorted by helpfulness to evaluate and compare the accuracy of these 3 models at feature level.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top 40 most helpful reviews \n",
    "helpful_reviews_indexes = new_reviews['helpfulVotes'].sort_values(ascending=False).head(40).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpful_reviews = new_reviews.loc[helpful_reviews_indexes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpful_reviews.to_csv(\"../data/helpful_reviews.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way it was evaluated was, if a review has 4 features. If the model predicts the sentiment of one feature correctly, the score give would be 0.25. If the sentiments of all features are predicted correctly within one review, the score for that particular review would be 1. The scores for 40 reviews were then added up and the sum was divided by 40 to get the % accuracy. The evaluation of each row was done on google spreadsheet which has also been uploaded on the data folder. It is **named \"helpful_reviews_evaluated.csv\"**\n",
    "\n",
    "Here is the result:\n",
    "\n",
    "VADER's score was 28.27/40 = **70.7%**  \n",
    "Logistic Regression's score was 28.57/40 = **71.4%**  \n",
    "BERT's score was 34.52/40 = **86.3%**  \n",
    "\n",
    "\n",
    "From the result above, we can clearly see BERT did a lot better than the other two on feature level. On the previous notebook, BERT has also shown to be superior to Logistic Regression in predicting overall rating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as I mentioned in the introduction of this project, we want to try to create a model that is able to detect negation effect on the sentence. BERT seems to be able to do it pretty well, as seen on the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      "I don't like the screen. It is different from the description. Please don't buy it.\n",
      "\n",
      "\n",
      "VADER rating: [(5, 'screen')]\n",
      "Logistic Regression rating: [(5, 'screen')]\n",
      "BERT rating: [(1.0, 'screen')]\n"
     ]
    }
   ],
   "source": [
    "example= \"I don't like the screen. It is different from the description. Please don't buy it.\"\n",
    "print(f\"Example: \\n{example}\")\n",
    "print(\"\\n\")\n",
    "print(f\"VADER rating: {vader_sentiments(example)}\")\n",
    "print(f\"Logistic Regression rating: {logreg_sentiments(example)}\")\n",
    "print(f\"BERT rating: {bert_sentiments(example)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that **BERT is able to detect the negation factor here correctly** while the other two models are unable to. Another observation is that BERT is able to read the context of the sentence better than the other two. Let's look at another example from Amazon review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon example: \n",
      "Eh wouldn’t buy again One - It comes in a weird box Two it had more scuffs and scratches than I’d like for the price 3 - I had to return it because of how over all lame it came and how it showed up/ battery was pretty warn/scratches/ and I didn’t get the awesome feeling of unboxing it..\n",
      "\n",
      "\n",
      "VADER rating: [(5, 'battery')]\n",
      "Logistic Regression rating: [(5, 'battery')]\n",
      "BERT rating: [(1.0, 'battery')]\n"
     ]
    }
   ],
   "source": [
    "example= new_reviews.loc[20317,\"reviews\"]\n",
    "\n",
    "\n",
    "print(f\"Amazon example: \\n{example}\")\n",
    "print(\"\\n\")\n",
    "print(f\"VADER rating: {vader_sentiments(example)}\")\n",
    "print(f\"Logistic Regression rating: {logreg_sentiments(example)}\")\n",
    "print(f\"BERT rating: {bert_sentiments(example)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the review above, we can see that battery was rated negatively. It was described as \"pretty warn/scratches\". However, VADER and Logistic Regression predicted this as a positive statement. It could be due to the word \"pretty\". It may be reading pretty in the wrong context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have deployed the feature extraction and sentiment analysis portion of this project here: http://ec2-3-22-98-206.us-east-2.compute.amazonaws.com:5000/predict-review\n",
    "\n",
    "The current purpose of deploying the model is just to showcase how the model works, especially to non-technical people who will not be equipped to download this notebook to try the model. However, we can definitely optimise the model and deploy it in any companies that receive a large volume of user-generated reviews so that it can **automatically extract the features along with the ratings of the features.** However, the current model has been fine-tuned to cell phone reviews, future adoptions to other type of reviews has to be fine-tuned again before deployment. \n",
    "\n",
    "As BERT model outperformed VADER and Logistic Regression, I have deployed BERT model to AWS. I wanted to deploy it to Heroku as it was free, however, the fine-tuned BERT model file size is too big to be deployed to Heroku. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, the BERT model performed relatively well in terms of accuracy score. However, I have found some limitations as I was testing the model with many variations of sentences with different sentiments. The limitations are elaborated below. \n",
    "\n",
    "**Limitations #1 (sentiment predictions):**\n",
    "\n",
    "It is predicting words like \"okay\" to be 5 star (To me, I would rate it as 3 or 4 star). This could be because many reviewers who think that certain features are \"okay\" have given a rating of 4 (which is a slightly above average type of rating). As we only fine-tuned the model with 3 classes (negative, neutral, positive), we have categoried 4 star rating to be positive. This explains why the model is giving a 5 star for feature that is described as \"okay\". \n",
    "\n",
    "**Solution #1:** \n",
    "\n",
    "I will consider fine-tuning the model with 5 classes instead as a way to improve the model. Also, I would like to consolidate all the frequently used words/sentences that are not accurately being predicted, label them with the right ratings and add them to the train dataset to fine-tune the model. \n",
    "\n",
    "\n",
    "**Limitations #2 (feature extractions):**\n",
    "\n",
    "Currently, it is only to search for features with the keywords that I have defined which is pretty limited. It is unable to search for synonym of the keywords. Example, \"this phone takes nice picture\" will not be picked up by the model currently. \n",
    "\n",
    "**Solution #2:** \n",
    "\n",
    "A simple way to improve this is to generate all the synonyms of the features and fine-tune the model further. As fine-tuning of BERT model is quite time-consuming, I will work on this improvement in the future.\n",
    "\n",
    "\n",
    "Recapping on the business problem and problem statement that we were trying to address: \n",
    "\n",
    "Business agenda: Improve user interface and platform experience by seggregating reviews into topics or summarising long reviews into just the main points/features and their corresponding sentiments.\n",
    "\n",
    "Problem Statement:\n",
    "1. Automatically segregates reviews by topics\n",
    "2. Summarise each review by features and sentiments\n",
    "\n",
    "\n",
    "So far, we have addressed both the problem statements. **The topic modelling that was done in notebook 2 and 3 have identified clear and logical topic clusters** which is definitely useful in segregating reviews on e-commerce platform (eg. Amazon,Lazada,Expedia,Airbnb), especially on popular listing with hundreds/thousands of reviews. This would greatly **enhance user experience.** The **extractions of features and sentiments would also help users extract important information from extremely long reviews, as we have seen on the EDA that there are reviews with about 1000 wordcounts.** \n",
    "\n",
    "\n",
    "With the output of features and ratings of each review that we have currently, we can also use it to produce an aggregated rating of each feature within a listing. Example, on iphone XR, the aggregated rating on camera is x, the aggregated rating on screen is y, the aggregated rating on battery is z (calculated from thousands of reviews that it has)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "6_comparisons_of_sentiment_analysis_results.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
