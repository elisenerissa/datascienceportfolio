{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For introduction and problem statement, please refer to notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook 1: 1_cellphones_reviews_data_cleaning_and_eda**\n",
    "- Data Import and Cleaning\n",
    "- Exploratory Data Analysis\n",
    "- Text Data Pre-processing\n",
    "\n",
    "**Notebook 2: 2_cellphones_reviews_topic modelling**\n",
    "- Data Import\n",
    "- Topic Modelling with Gensim\n",
    "\n",
    "**Notebook 3: 3_cellphones_reviews_topic_analysis_and_visualizations**\n",
    "- Findings and Analysis of Topic Modelling\n",
    "\n",
    "**Notebook 4: 4_features_extractions_and_sentiment_analysis**\n",
    "- [Data Import](#Data-Import)\n",
    "- [Sentiment Analysis with VADER](#Sentiment-Analysis-with-VADER)\n",
    "- [entiment Analysis with Logistic Regression(Multi-Class Classification)](#Sentiment-Analysis-with-Logistic-Regression-Classifier)\n",
    "- [Evaluation of Sentiment Analysis with BERT(Multi-Class Classification)](#Evaluation-of-Sentiment-Analysis-with-BERT)   \n",
    "Please refer to notebook 5 for the fine-tuning process of pre-trained BERT model\n",
    "- Comparison of the 3 Methods \n",
    "- Recommendation and Conclusion \n",
    "- Future Steps\n",
    "\n",
    "**Notebook 5: fine_tuning_of_BERT_model**   \n",
    "The reason why this notebook is separated from notebook 4 which contains the evaluation of BERT model is because the fine-tuning of BERT model requires GPU. Hence, the model was fine-tuned on Google Colaboratory and loaded back into notebook 4 for evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = pd.read_csv('../data/cleaned_combined_data.csv',na_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = {\n",
    "    'new': 3.0\n",
    "}\n",
    "\n",
    "analyser.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_words = ['ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "\"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
    "'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "'wouldn', \"wouldn't\",\"not\",\"no\",'don',\"don't\"]\n",
    "\n",
    "for word in negation_words:\n",
    "    stop_words.remove(word)\n",
    "\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_with_keywords (reviews):\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen','fingerprint','fingerprints',\n",
    "                        'ringtones','charger']\n",
    "    summary = set()\n",
    "    texts = tokenize.sent_tokenize(reviews)\n",
    "    for sentence in texts:\n",
    "        sentence = sentence.lower()\n",
    "        for word in list_of_keywords:\n",
    "            if word in sentence:\n",
    "                summary.add(sentence)\n",
    "                \n",
    "    return list(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_reviews (reviews):\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen','fingerprint','fingerprints',\n",
    "                        'ringtones','charger']\n",
    "    summary = set()\n",
    "    texts = tokenize.sent_tokenize(reviews)\n",
    "    for sentence in texts:\n",
    "        sentence = sentence.lower()\n",
    "        for word in list_of_keywords:\n",
    "            if word in sentence:\n",
    "                # Remove HTML.\n",
    "                post_text = BeautifulSoup(sentence).get_text()\n",
    "\n",
    "                # Remove non-letters.\n",
    "                letters_only = ' '.join(re.findall(r\"[A-zâ€™]+\",post_text))\n",
    "\n",
    "                # Convert to lower case, split into individual words.\n",
    "                words = letters_only.lower().split()\n",
    "\n",
    "                #convert the stopwords to a set.\n",
    "                stops = set(stop_words)\n",
    "\n",
    "                # Remove stopwords.\n",
    "                meaningful_words = [w for w in words if w not in stops]\n",
    "\n",
    "                # Stemming \n",
    "                #p_stemmer = PorterStemmer()\n",
    "                #meaningful_words = [p_stemmer.stem(w) for w in meaningful_words]\n",
    "\n",
    "                #Lemmatize\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                meaningful_words = [lemmatizer.lemmatize(word) for word in meaningful_words]\n",
    "\n",
    "                cleaned_sentence = (\" \".join(meaningful_words))\n",
    "                \n",
    "                summary.add(cleaned_sentence)\n",
    "                \n",
    "    return list(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_sentiments (summarised_reviews):\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen','fingerprint','fingerprints',\n",
    "                        'ringtones','charger']\n",
    "    summary = set()\n",
    "    \n",
    "    for cleaned_sentence in summarised_reviews:\n",
    "        \n",
    "        for word in list_of_keywords:\n",
    "            if word in cleaned_sentence:\n",
    "                score = analyser.polarity_scores(cleaned_sentence)\n",
    "                compound = score['compound']\n",
    "\n",
    "                if compound >= 0.075:\n",
    "                    sentiment_score = 5\n",
    "                elif compound >= 0.05:\n",
    "                    sentiment_score = 4\n",
    "                elif compound <= -0.075:\n",
    "                    sentiment_score = 1\n",
    "                elif compound <= -0.05:\n",
    "                    sentiment_score = 2\n",
    "                else:\n",
    "                    sentiment_score = 3\n",
    "\n",
    "                summary.add((sentiment_score,word))\n",
    "    return list(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews['summary'] = new_reviews['reviews'].apply(summarise_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews['sentences_with_keywords'] = new_reviews['reviews'].apply(sentences_with_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews['features_and_sentiments'] = new_reviews['summary'].apply(features_and_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)\n",
    "new_reviews[['reviews','summary']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews[\"filter summary\"] = new_reviews['summary'].apply(lambda x: x != [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = new_reviews[new_reviews[\"filter summary\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_reviews.to_csv('../data/cleaned_combined_data_with_keywords.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iphone_xs = new_reviews[new_reviews['asin'] == 'B07RT1X4FJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth',None)\n",
    "#iphone_xs['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iphone_xs['summary'][63325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth',None)\n",
    "#iphone_xs['features_and_sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iphone_xs.loc[63202,'features_and_sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews['multi_class_sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature and target variable\n",
    "X = new_reviews['cleaned_reviews']\n",
    "y = new_reviews['multi_class_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = y_train.value_counts(normalize=True)\n",
    "baseline_accuracy = round(baseline_model[2],3)\n",
    "\n",
    "print(f\"Baseline accuracy: {baseline_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the pipeline\n",
    "lr_cvec_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr',LogisticRegression(random_state=42,solver='liblinear', max_iter=10000))\n",
    "])\n",
    "\n",
    "#create hyperparameters for gridsearch\n",
    "lr_cvec_params = {\n",
    "    'cvec__max_features': [3000,4000,5000],\n",
    "    'cvec__min_df':[2,3],\n",
    "    'cvec__max_df':[0.9,0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__C':[0.01,0.1,1],\n",
    "    'lr__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "lr_cvec_gs = GridSearchCV(lr_cvec_pipe, # what object are we optimizing?\n",
    "                  param_grid=lr_cvec_params , # what parameters values are we searching?\n",
    "                  cv=5,\n",
    "                 n_jobs=-1,verbose=1) \n",
    "\n",
    "#fit the model\n",
    "lr_cvec_gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy_score = round(lr_cvec_gs.score(X_train,y_train),3)\n",
    "testing_accuracy_score = round(lr_cvec_gs.score(X_test,y_test),3)\n",
    "\n",
    "print(f\"Logistic Regression CVEC Train Accuracy Score: {training_accuracy_score}\")\n",
    "print(f\"Logistic Regression CVEC Test Accuracy Score: {testing_accuracy_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename= '../data/logreg_3classes.pkl'\n",
    "pickle.dump(lr_cvec_gs,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on Feature Level with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_classification (reviews):\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen','fingerprint','fingerprints',\n",
    "                        'ringtones','charger']\n",
    "    summary = set()\n",
    "    pred = logreg_model.predict(reviews)\n",
    "    \n",
    "    \n",
    "    for i,cleaned_sentence in enumerate(reviews):        \n",
    "        for word in list_of_keywords:\n",
    "            if word in cleaned_sentence:\n",
    "                summary.add((pred[i],word))\n",
    "                \n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews['logreg_pred'] = new_reviews['summary'].apply(logreg_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Sentiment Analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_reviews['multi_class_sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(new_reviews.index.values,\n",
    "                                                  new_reviews.multi_class_sentiment.values,\n",
    "                                                  test_size = 0.25,\n",
    "                                                  random_state= 42,\n",
    "                                                  stratify=new_reviews.multi_class_sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews['data_type'] = ['not_set']*new_reviews.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews.loc[X_train, 'data_type'] = 'train'\n",
    "new_reviews.loc[X_val, 'data_type'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews.groupby(['rating', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data in a format that is readable by BERT\n",
    "#since we are not doing fine-tuning of the train data here\n",
    "#we will only prepare the validation data for evaluation\n",
    "#the full-fine tuning process including tokening and dataloader \n",
    "#of train dataset is available in notebook 5\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    new_reviews[new_reviews.data_type=='val'].reviews.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val= torch.tensor(new_reviews[new_reviews.data_type=='val'].multi_class_sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up BERT Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    #label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    correct_pred = 0\n",
    "    total_count = 0\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "        \n",
    "        correct_pred = correct_pred + len(y_preds[y_preds==label])\n",
    "        total_count = total_count + len(y_true)\n",
    "        \n",
    "    print(f'Total Accuracy:{correct_pred/total_count}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader_val):\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../data/finetuned_BERT_epoch_2_3classes.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on Feature Level with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tokenizer and Encoding Data by Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sentiments (summarised_reviews):\n",
    "    list_of_keywords = ['camera','screen','battery','simcard','touchscreen','fingerprint','fingerprints',\n",
    "                        'ringtones','charger']\n",
    "    \n",
    "    summary = set()\n",
    "    \n",
    "    encoded_data_features = tokenizer.batch_encode_plus(\n",
    "    summarised_reviews, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "    input_ids_features = encoded_data_features['input_ids']\n",
    "    attention_masks_features = encoded_data_features['attention_mask']\n",
    "    #labels_features = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "    dataset_features = TensorDataset(input_ids_features, attention_masks_features)\n",
    "\n",
    "    dataloader_features = DataLoader(dataset_features , \n",
    "                                       sampler=SequentialSampler(dataset_features ), \n",
    "                                       batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    for batch in dataloader_features:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                     }\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        \n",
    "    \n",
    "    rating_score = torch.argmax(outputs[0],dim=1)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        predicted_ratings = []\n",
    "\n",
    "        for score in rating_score:\n",
    "            if float(score) == 2.0:\n",
    "                rating = 5\n",
    "                predicted_ratings.append(rating)\n",
    "            elif float(score) == 1.0:\n",
    "                rating = 3\n",
    "                predicted_ratings.append(rating)\n",
    "            else:\n",
    "                rating = 1\n",
    "                predicted_ratings.append(rating)\n",
    "\n",
    "        for i,cleaned_sentence in enumerate(summarised_reviews):        \n",
    "            for word in list_of_keywords:\n",
    "                if word in cleaned_sentence:\n",
    "                    summary.add((float(predicted_ratings[i]),word))\n",
    "    except:\n",
    "        summary.add(np.nan)\n",
    "        \n",
    "                \n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "bert_sentiments(new_reviews.sentences_with_keywords.values[5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews['bert_analysis'] = new_reviews['sentences_with_keywords'].progress_apply(bert_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean ratings by features of each unique product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_asins = new_reviews['asin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews.loc[1,'features_and_sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = {}\n",
    "\n",
    "#for cell in new_review['features_and_sentiments']: \n",
    "all_features=set()\n",
    "\n",
    "for product in unique_asins:\n",
    "    all_products[product] = {'camera':[],'battery':[],'fingerprint':[],'screen':[],'charger':[]}\n",
    "    for idx in new_reviews.index:\n",
    "        if new_reviews.loc[idx,'asin'] == product:\n",
    "            for feature in new_reviews.loc[idx,'features_and_sentiments']:\n",
    "                all_features.add(feature[1])\n",
    "                if feature[1] =='battery':\n",
    "                    all_products[product]['battery'].append(feature[0])\n",
    "                elif feature[1]  == 'camera':\n",
    "                    all_products[product]['camera'].append(feature[0])\n",
    "                elif feature[1]  == 'charger':\n",
    "                    all_products[product]['charger'].append(feature[0])\n",
    "                elif feature[1] == 'screen':\n",
    "                    all_products[product]['screen'].append(feature[0])\n",
    "                elif feature[1] == 'fingerprint':\n",
    "                    all_products[product]['fingerprint'].append(feature[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_1,value_1 in all_products.items():\n",
    "    for key_2,value_2 in all_products[key_1].items():\n",
    "        try:\n",
    "            all_products[key_1][key_2] = round(np.mean(all_products[key_1][key_2]),1)\n",
    "        except:\n",
    "            all_products[key_1][key_2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ratings = pd.DataFrame(all_products).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ratings.reset_index(inplace=True)\n",
    "mean_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ratings.rename(columns={'index':'asin'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_mean_ratings = pd.merge(mean_ratings,new_reviews[['asin','item_title']],on='asin',how='inner')\n",
    "updated_mean_ratings.drop_duplicates(subset=['asin'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_mean_ratings.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_mean_ratings.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean ratings by features of each unique product\n",
    "\n",
    "new_reviews.reset_index(inplace=True,drop=True)\n",
    "\n",
    "unique_asins = new_reviews['asin'].unique()\n",
    "\n",
    "new_reviews.loc[1,'features_and_sentiments']\n",
    "\n",
    "all_products = {}\n",
    "\n",
    "#for cell in new_review['features_and_sentiments']: \n",
    "all_features=set()\n",
    "\n",
    "for product in unique_asins:\n",
    "    all_products[product] = {'camera':[],'battery':[],'fingerprint':[],'screen':[],'charger':[]}\n",
    "    for idx in new_reviews.index:\n",
    "        if new_reviews.loc[idx,'asin'] == product:\n",
    "            for feature in new_reviews.loc[idx,'features_and_sentiments']:\n",
    "                all_features.add(feature[1])\n",
    "                if feature[1] =='battery':\n",
    "                    all_products[product]['battery'].append(feature[0])\n",
    "                elif feature[1]  == 'camera':\n",
    "                    all_products[product]['camera'].append(feature[0])\n",
    "                elif feature[1]  == 'charger':\n",
    "                    all_products[product]['charger'].append(feature[0])\n",
    "                elif feature[1] == 'screen':\n",
    "                    all_products[product]['screen'].append(feature[0])\n",
    "                elif feature[1] == 'fingerprint':\n",
    "                    all_products[product]['fingerprint'].append(feature[0])\n",
    "        \n",
    "\n",
    "for key_1,value_1 in all_products.items():\n",
    "    for key_2,value_2 in all_products[key_1].items():\n",
    "        try:\n",
    "            all_products[key_1][key_2] = round(np.mean(all_products[key_1][key_2]),1)\n",
    "        except:\n",
    "            all_products[key_1][key_2] = np.nan\n",
    "\n",
    "mean_ratings = pd.DataFrame(all_products).T\n",
    "\n",
    "mean_ratings.reset_index(inplace=True)\n",
    "mean_ratings\n",
    "\n",
    "mean_ratings.rename(columns={'index':'asin'},inplace=True)\n",
    "\n",
    "updated_mean_ratings = pd.merge(mean_ratings,new_reviews[['asin','item_title']],on='asin',how='inner')\n",
    "updated_mean_ratings.drop_duplicates(subset=['asin'],keep='first',inplace=True)\n",
    "\n",
    "updated_mean_ratings.reset_index(inplace=True,drop=True)\n",
    "\n",
    "updated_mean_ratings.tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
